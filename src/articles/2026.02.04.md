---
id: edition-2026-02-04-ai-infrastructure
date: 2026-02-04
title: "AI Infrastructure Wars: From Chips to Chatbots"
summary: "ChatGPT's market dominance, Google's TPU v7 Ironwood challenge, GPT-5's unified architecture, and the rise of Chinese MoE models."
---

## The State of AI Chatbots: January 2026

![AI Chatbot Market Share Worldwide - January 2026](/weekly-screenshots/2026.02.04/ai-chatbot-market-share.png)

ChatGPT continues to dominate the AI chatbot space with a commanding **80.14%** market share as of January 2026. The competition trails significantly:

- **Perplexity**: 8.15%
- **Google Gemini**: 7.19%
- **Microsoft Copilot**: 3.62%
- **Claude**: 0.9%
- **DeepSeek**: 0.01%

Despite massive investments from Google, Microsoft, and Anthropic, OpenAI's first-mover advantage and continuous iteration have created an almost insurmountable lead in consumer adoption.

---

## Google's TPU Evolution: A Decade of Custom Silicon

![Google TPU Architecture Evolution and Scaling Timeline](/weekly-screenshots/2026.02.04/google-tpu-evolution.png)

Google's Tensor Processing Units have evolved dramatically since their 2015 debut:

| Generation | Year | Peak Performance | Max Chips/Pod |
|------------|------|------------------|---------------|
| v1 | 2015 | 92 TOPS (int8) | 1 |
| v2 | 2017 | 45 tflops | 256 |
| v3 | 2018 | 123 tflops | 1,024 |
| v4 | 2021 | 275 tflops | 4,096 |
| v5p | 2023 | 459 tflops | 8,960 |
| v6 (Trillium) | 2024 | 918 tflops | 256* |
| **v7 (Ironwood)** | 2025 | **4,614 tflops** | **9,216** |

The v7 Ironwood represents a massive leap—an "inference-first" design with 6× HBM capacity and 5× the performance of its predecessor.

---

## GPT-5: OpenAI's Unified Intelligence System

![OpenAI Introducing GPT-5 - August 2025](/weekly-screenshots/2026.02.04/openai-gpt5-announcement.png)

Released August 7, 2025, GPT-5 represents a architectural paradigm shift. Rather than separate models, it's **one unified system** with:

- A **smart, efficient model** for most questions
- A **deeper reasoning model** (GPT-5 thinking) for harder problems
- A **real-time router** that decides which to invoke based on:
  - Conversation type
  - Complexity
  - Tool needs
  - Explicit user intent ("think hard about this")

The router continuously trains on user behavior—including when users manually switch models, preference rates, and measured correctness.

---

## The Hardware Race: TPU vs NVIDIA

![TFLOPs and System Availability of TPU vs Nvidia (BF16 Dense)](/weekly-screenshots/2026.02.04/tpu-vs-nvidia-tflops.png)

The AI accelerator battle has intensified. While NVIDIA's GB200 (2025) leads at ~2,500 TFLOPs, Google's TPU v7 is closing the gap at ~2,300 TFLOPs.

Key milestones:
- **A100** (2020): ~300 TFLOPs
- **H100** (2022): ~1,000 TFLOPs
- **GB200** (2025): ~2,500 TFLOPs
- **TPU v7** (2025): ~2,300 TFLOPs

Google's vertically integrated approach—owning both the models (Gemini) and the hardware (TPU)—gives them a unique cost and optimization advantage that external competitors can't match.

---

## MoE Architecture Wars: DeepSeek vs Kimi K2

![DeepSeek V3/R1 vs Kimi K2 Architecture Comparison](/weekly-screenshots/2026.02.04/deepseek-vs-kimi-architecture.png)

Chinese AI labs are pushing Mixture-of-Experts architectures to new scales:

**DeepSeek V3/R1 (671B)**
- 128 attention heads
- 256 experts with 8 active per token
- Only 37B parameters active per inference
- 129k vocabulary, 128k context

**Kimi K2 (1 Trillion)**
- 64 attention heads
- 384 experts with 8 active per token
- Only 32B parameters active per inference
- 160k vocabulary, 128k context

Both use Multi-head Latent Attention (MLA) and SwiGLU feed-forward modules, but take different scaling approaches: DeepSeek opts for more heads with fewer experts, while Kimi K2 goes for more experts with fewer heads.

---

## Inside a TPU v7 Ironwood Superpod

![TPU v7 Ironwood Superpod - 9,216 chips](/weekly-screenshots/2026.02.04/tpu-v7-ironwood-superpod.png)

This is what training frontier AI looks like: a TPU v7 Ironwood superpod directly connecting **9,216 Ironwood TPUs** in a single domain. Google claims this infrastructure can train models like Gemini 3 entirely without NVIDIA hardware.

The 7th generation TPU represents Google's most aggressive challenge yet to NVIDIA's dominance in AI compute.

---

*Data sources: Statcounter, SemiAnalysis, Sebastian Raschka, Google, OpenAI*
